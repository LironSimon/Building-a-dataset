{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Writing new funcs for AI Feynman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8f44f7f12d3494ebb748be200314ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fd1e1ce2cb24307b6f47567c755329e",
              "IPY_MODEL_627ff7447dfc4305b516a6684ed964cd",
              "IPY_MODEL_ad75f63be72047b58bdf37ad45ed8387"
            ],
            "layout": "IPY_MODEL_8d82b82a2e064dcc8916e12c95c86883"
          }
        },
        "1fd1e1ce2cb24307b6f47567c755329e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80adfe8054ce47a98d88d956e6fb14ee",
            "placeholder": "​",
            "style": "IPY_MODEL_232bcd9550b245c69a7a3444ad383593",
            "value": "Calculating...epoch2/3: 100%"
          }
        },
        "627ff7447dfc4305b516a6684ed964cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d1bb6dc1f342fa99c919a2c9bc06df",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30f23f6ad698465bb648d6fbd4b6712c",
            "value": 3
          }
        },
        "ad75f63be72047b58bdf37ad45ed8387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c1f9f040a034676bbf6bfe439e318ae",
            "placeholder": "​",
            "style": "IPY_MODEL_263322a3f8c6477ab7eefb43e6e85f68",
            "value": " 3/3 [49:59&lt;00:00, 988.96s/it]"
          }
        },
        "8d82b82a2e064dcc8916e12c95c86883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80adfe8054ce47a98d88d956e6fb14ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "232bcd9550b245c69a7a3444ad383593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1d1bb6dc1f342fa99c919a2c9bc06df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f23f6ad698465bb648d6fbd4b6712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c1f9f040a034676bbf6bfe439e318ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "263322a3f8c6477ab7eefb43e6e85f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LironSimon/Prediction-of-Projectile-Landing-Points-with-ML/blob/main/Writing_new_funcs_for_AI_Feynman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir7_gq53qkUC"
      },
      "source": [
        "# Writing changes to AI Feynman 2.0 code\n",
        "So that their SimpleNet design can be swapt with a different NN of my choosing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Lh5Lakqo_l"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um22QVI3WlJT",
        "outputId": "a285869a-a928-4494-b3ed-42da3447133d"
      },
      "source": [
        "!pip install aifeynman"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting aifeynman\n",
            "  Downloading aifeynman-2.0.7.tar.gz (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aifeynman) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from aifeynman) (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from aifeynman) (0.11.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from aifeynman) (0.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from aifeynman) (2.4.0)\n",
            "Requirement already satisfied: sympy>=1.4 in /usr/local/lib/python3.7/dist-packages (from aifeynman) (1.7.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from aifeynman) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from aifeynman) (0.10.0+cu102)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy>=1.4->aifeynman) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->aifeynman) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aifeynman) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aifeynman) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aifeynman) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aifeynman) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->aifeynman) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->aifeynman) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->aifeynman) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->aifeynman) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->aifeynman) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->aifeynman) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->aifeynman) (7.1.2)\n",
            "Building wheels for collected packages: aifeynman\n",
            "  Building wheel for aifeynman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aifeynman: filename=aifeynman-2.0.7-cp37-cp37m-linux_x86_64.whl size=863107 sha256=073aa8007cf264ff1466bc94aeb7cc8aac69d0850be9122cb63e6ffeb6252585\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/51/dc/f385966c690f4ae7b645205b5effcb67fd6eccef022053b0fe\n",
            "Successfully built aifeynman\n",
            "Installing collected packages: aifeynman\n",
            "Successfully installed aifeynman-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM1PRqSSOBUB",
        "outputId": "b41b063a-964b-4e67-dd75-a00a5498ccd8"
      },
      "source": [
        "pip install torchdiffeq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RPLiAbnXIXHm",
        "outputId": "a57f2395-b956-43b5-da5e-75e694d09aa4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm.notebook as tq\n",
        "\n",
        "## Import the Adjoint Method (ODE Solver)\n",
        "# import torchdiffeq\n",
        "# from torchdiffeq import odeint_adjoint as odeint\n",
        "\n",
        "import aifeynman\n",
        "\n",
        "# check device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GZel5g8Rg2L"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPJfyIMgKVK6"
      },
      "source": [
        "##########################################\n",
        "# ACTION NEEDED! Change to a relevant path\n",
        "##########################################\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Deep Learning/Final Project/Datasets/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOMVe1q4yGO1"
      },
      "source": [
        "## **The ODE NN**\n",
        "The ODE Network is presented in the paper 'Neural Ordinary Differential Equations', 2018. In this section I write working examples of it, accourding to the description in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3og6NU-yJOY"
      },
      "source": [
        "### Untouched Code\n",
        "This code does not run as it somewhat of a psodu code, written for explanational purpouses. It can be found [here](https://github.com/llSourcell/Neural_Differential_Equations/blob/master/Neural_Ordinary_Differential_Equations.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "5m0m4bpoyn1O",
        "outputId": "5450f6be-fd8d-433f-c1cc-ac141a9e6f64"
      },
      "source": [
        "'''\n",
        "## Normal Residual Block\n",
        "class ResBlock(nn.Module):\n",
        "    #init a block - Convolve, pool, activate, repeat\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.norm1 = norm(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.norm2 = norm(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    #Forward pass - pass output of one layer to the input of the next \n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        out = self.relu(self.norm1(x))\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + shortcut\n",
        "\n",
        "## Ordinary Differential Equation Definition     \n",
        "class ODEfunc(nn.Module):\n",
        "    # init ODE variables\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = norm(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(dim, dim)\n",
        "        self.norm2 = norm(dim)\n",
        "        self.conv2 = conv3x3(dim, dim)\n",
        "        self.norm3 = norm(dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    # init ODE operations \n",
        "    def forward(self, t, x):\n",
        "      #nfe = number of function evaluations per timestep\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        " ## ODE block\n",
        "class ODEBlock(nn.Module):\n",
        "    #initialized as an ODE Function\n",
        "    #count the time\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "    #foorward pass \n",
        "    #input the ODE function and input data into the ODE Solver (adjoint method)\n",
        "    # to compute a forward pass\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n",
        "        return out[1]\n",
        "\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value\n",
        "\n",
        "\n",
        "## Main Method\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    #Add Pooling\n",
        "    downsampling_layers = [\n",
        "         nn.Conv2d(1, 64, 3, 1),\n",
        "         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "     ]\n",
        "\n",
        "    # Initialize the network as 1 ODE Block\n",
        "    feature_layers = [ODEBlock(ODEfunc(64))] \n",
        "    # Fully connected Layer at the end\n",
        "    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n",
        "  \n",
        "    #The Model consists of an ODE Block, pooling, and a fully connected block at the end\n",
        "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n",
        "\n",
        "    #Declare Gradient Descent Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
        "\n",
        "    #Training Loop\n",
        "    for itr in range(args.nepochs * batches_per_epoch):\n",
        "\n",
        "        \n",
        "        #init the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Generate training data\n",
        "        x, y = data_gen()\n",
        "        #Input Training data to model, get Prediction\n",
        "        logits = model(x)\n",
        "        #Compute Error using Prediction vs Actual Label\n",
        "        loss = CrossEntropyLoss(logits, y)\n",
        "        \n",
        "        #Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n## Normal Residual Block\\nclass ResBlock(nn.Module):\\n    #init a block - Convolve, pool, activate, repeat\\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n        super(ResBlock, self).__init__()\\n        self.norm1 = norm(inplanes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.downsample = downsample\\n        self.conv1 = conv3x3(inplanes, planes, stride)\\n        self.norm2 = norm(planes)\\n        self.conv2 = conv3x3(planes, planes)\\n\\n    #Forward pass - pass output of one layer to the input of the next \\n    def forward(self, x):\\n        shortcut = x\\n        out = self.relu(self.norm1(x))\\n        out = self.conv1(out)\\n        out = self.norm2(out)\\n        out = self.relu(out)\\n        out = self.conv2(out)\\n\\n        return out + shortcut\\n\\n## Ordinary Differential Equation Definition     \\nclass ODEfunc(nn.Module):\\n    # init ODE variables\\n    def __init__(self, dim):\\n        super(ODEfunc, self).__init__()\\n        self.norm1 = norm(dim)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.conv1 = conv3x3(dim, dim)\\n        self.norm2 = norm(dim)\\n        self.conv2 = conv3x3(dim, dim)\\n        self.norm3 = norm(dim)\\n        self.nfe = 0\\n\\n    # init ODE operations \\n    def forward(self, t, x):\\n      #nfe = number of function evaluations per timestep\\n        self.nfe += 1\\n        out = self.norm1(x)\\n        out = self.relu(out)\\n        out = self.conv1(out)\\n        out = self.norm2(out)\\n        out = self.relu(out)\\n        out = self.conv2(out)\\n        out = self.norm3(out)\\n        return out\\n\\n\\n ## ODE block\\nclass ODEBlock(nn.Module):\\n    #initialized as an ODE Function\\n    #count the time\\n    def __init__(self, odefunc):\\n        super(ODEBlock, self).__init__()\\n        self.odefunc = odefunc\\n        self.integration_time = torch.tensor([0, 1]).float()\\n\\n    #foorward pass \\n    #input the ODE function and input data into the ODE Solver (adjoint method)\\n    # to compute a forward pass\\n    def forward(self, x):\\n        self.integration_time = self.integration_time.type_as(x)\\n        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n        return out[1]\\n\\n    @property\\n    def nfe(self):\\n        return self.odefunc.nfe\\n\\n    @nfe.setter\\n    def nfe(self, value):\\n        self.odefunc.nfe = value\\n\\n\\n## Main Method\\nif __name__ == '__main__':\\n  \\n    #Add Pooling\\n    downsampling_layers = [\\n         nn.Conv2d(1, 64, 3, 1),\\n         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n     ]\\n\\n    # Initialize the network as 1 ODE Block\\n    feature_layers = [ODEBlock(ODEfunc(64))] \\n    # Fully connected Layer at the end\\n    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n  \\n    #The Model consists of an ODE Block, pooling, and a fully connected block at the end\\n    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\\n    #Declare Gradient Descent Optimizer\\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\\n    #Training Loop\\n    for itr in range(args.nepochs * batches_per_epoch):\\n\\n        \\n        #init the optimizer\\n        optimizer.zero_grad()\\n        \\n        #Generate training data\\n        x, y = data_gen()\\n        #Input Training data to model, get Prediction\\n        logits = model(x)\\n        #Compute Error using Prediction vs Actual Label\\n        loss = CrossEntropyLoss(logits, y)\\n        \\n        #Backpropagate\\n        loss.backward()\\n        optimizer.step()\\n\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi5LOpO1y1id"
      },
      "source": [
        "### A Working ODE Example on matrices\n",
        "Here the code is written in full, so that it can work. As in the original, it is meant to work on data in the form of matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBhPvxfo0lT9"
      },
      "source": [
        "**1. Creating the ODE Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igkgW-7R-Hks"
      },
      "source": [
        "## Ordinary Differential Equation Definition     \n",
        "class ODEfunc(nn.Module):\n",
        "    # init ODE variables\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(dim, dim)\n",
        "        self.norm2 = nn.BatchNorm2d(dim)\n",
        "        self.conv2 = conv3x3(dim, dim)\n",
        "        self.norm3 = nn.BatchNorm2d(dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    # init ODE operations \n",
        "    def forward(self, t, x):\n",
        "      #nfe = number of function evaluations per timestep\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## ODE block\n",
        "class ODEBlock(nn.Module):\n",
        "    #initialized as an ODE Function\n",
        "    #count the time\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "    #foorward pass \n",
        "    #input the ODE function and input data into the ODE Solver (adjoint method)\n",
        "    # to compute a forward pass\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=1e-7, atol=1e-9)\n",
        "        return out[1].double()\n",
        "\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value\n",
        "\n",
        "\n",
        "## Normal Residual Block\n",
        "class ResBlock(nn.Module):\n",
        "    #init a block - Convolve, pool, activate, repeat\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.norm2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    #Forward pass - pass output of one layer to the input of the next \n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        out = self.relu(self.norm1(x))\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + shortcut\n",
        "\n",
        "\n",
        "# helpful nets that were used:\n",
        "class conv1x1(nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, stride=1, padding=1):\n",
        "        super(conv1x1, self).__init__()\n",
        "        self.conv = nn.Conv2d(inchannels, outchannels, 1, stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out \n",
        "\n",
        "class conv3x3(nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, stride=1, padding=1):\n",
        "        super(conv3x3, self).__init__()\n",
        "        self.conv = nn.Conv2d(inchannels, outchannels, 3, stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZEuRlTC0pr2"
      },
      "source": [
        "**2. Generate example data to test it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckIjzdWeKkPB"
      },
      "source": [
        "def data_gen():\n",
        "  # initialise lists of data ys=f(xs)\n",
        "  xs, ys = [],[]\n",
        " \n",
        "  for _ in range(16):\n",
        "    x= [random.uniform(0,1) for i in range(9)]  # list of 9 random floats from (0,1)\n",
        "    y = ( (x[0]-x[1])**2 + (x[2] - x[3])**2 )**0.5 #only the 4 first xs are used to create y\n",
        "    xs.append(x) #adds 9 xs\n",
        "    ys.append(y) #adds 1 y\n",
        "  \n",
        "  xs = np.array(xs, dtype=\"Float64\")\n",
        "  xs = xs.reshape(16,1,3,3)\n",
        "  ys = np.array(ys)\n",
        "\n",
        "  xs = torch.from_numpy(xs)\n",
        "  ys = torch.from_numpy(ys)\n",
        "\n",
        "  return xs, ys "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAoCGj6mP84C",
        "outputId": "544a24a4-96d1-4a8b-e3e8-b7878c7ac28d"
      },
      "source": [
        "xs, ys = data_gen()\n",
        "print(xs.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXuDISwA19in"
      },
      "source": [
        "**3. Create a new loss method.** The cross enthropy does not work here, so I used the same RMS method as in AI Feynman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNSpOdJFUVB9"
      },
      "source": [
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ejCX5G52OzR"
      },
      "source": [
        "**4. Varify that new ODE works** on the example data we created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649,
          "referenced_widgets": [
            "d8f44f7f12d3494ebb748be200314ecc",
            "1fd1e1ce2cb24307b6f47567c755329e",
            "627ff7447dfc4305b516a6684ed964cd",
            "ad75f63be72047b58bdf37ad45ed8387",
            "8d82b82a2e064dcc8916e12c95c86883",
            "80adfe8054ce47a98d88d956e6fb14ee",
            "232bcd9550b245c69a7a3444ad383593",
            "b1d1bb6dc1f342fa99c919a2c9bc06df",
            "30f23f6ad698465bb648d6fbd4b6712c",
            "6c1f9f040a034676bbf6bfe439e318ae",
            "263322a3f8c6477ab7eefb43e6e85f68"
          ]
        },
        "id": "l6bAuy_RBhhh",
        "outputId": "04670a35-110a-444e-8b07-d0f08168bf06"
      },
      "source": [
        "lr = 0.0001\n",
        "\n",
        "## Main Method\n",
        "\n",
        "#Add Pooling\n",
        "downsampling_layers = [\n",
        "         nn.Conv2d(1, 64, 3, 1),\n",
        "         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "        ]\n",
        "\n",
        "# Initialize the network as 1 ODE Block\n",
        "feature_layers = [ODEBlock(ODEfunc(64))]\n",
        "    \n",
        "# Fully connected Layer at the end\n",
        "fc_layers = [nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(64, 1)]\n",
        "    \n",
        "#The Model consists of an ODE Block, pooling, and a fully connected block at the end\n",
        "model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n",
        "#Declare Gradient Descent Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "model = model.double()\n",
        "\n",
        "#Training Loop\n",
        "epochs = 3 \n",
        "batches_per_epoch = 10\n",
        "\n",
        "pbar = tq.tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    pbar.set_description(f'Calculating...epoch{epoch}/{epochs}')\n",
        "    \n",
        "    for batch_id in range(batches_per_epoch):\n",
        "        print(f'Epoch{epoch}: Batch {batch_id}/{batches_per_epoch}')\n",
        "        #init the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Generate training data\n",
        "        x, y = data_gen()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        #Input Training data to model, get Prediction\n",
        "        logits = model(x.double())\n",
        "        #Compute Error using Prediction vs Actual Label\n",
        "        loss = rmse_loss(logits, y)\n",
        "        \n",
        "        #Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print( f\"epoch num {epoch}, loss: {loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8f44f7f12d3494ebb748be200314ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch0: Batch 0/10\n",
            "Epoch0: Batch 1/10\n",
            "Epoch0: Batch 2/10\n",
            "Epoch0: Batch 3/10\n",
            "Epoch0: Batch 4/10\n",
            "Epoch0: Batch 5/10\n",
            "Epoch0: Batch 6/10\n",
            "Epoch0: Batch 7/10\n",
            "Epoch0: Batch 8/10\n",
            "Epoch0: Batch 9/10\n",
            "epoch num 0, loss: 1.0081127402641592\n",
            "Epoch1: Batch 0/10\n",
            "Epoch1: Batch 1/10\n",
            "Epoch1: Batch 2/10\n",
            "Epoch1: Batch 3/10\n",
            "Epoch1: Batch 4/10\n",
            "Epoch1: Batch 5/10\n",
            "Epoch1: Batch 6/10\n",
            "Epoch1: Batch 7/10\n",
            "Epoch1: Batch 8/10\n",
            "Epoch1: Batch 9/10\n",
            "epoch num 1, loss: 0.832873714807804\n",
            "Epoch2: Batch 0/10\n",
            "Epoch2: Batch 1/10\n",
            "Epoch2: Batch 2/10\n",
            "Epoch2: Batch 3/10\n",
            "Epoch2: Batch 4/10\n",
            "Epoch2: Batch 5/10\n",
            "Epoch2: Batch 6/10\n",
            "Epoch2: Batch 7/10\n",
            "Epoch2: Batch 8/10\n",
            "Epoch2: Batch 9/10\n",
            "epoch num 2, loss: 0.5587498639868949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgzVt-kUrbmj"
      },
      "source": [
        "### A Working ODE Example on vectors.\n",
        "**Sets the New Net Class to replace SimpleNet in AI Feynman's code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GCwp5gdOTST"
      },
      "source": [
        "class ODEBlock(nn.Module):\n",
        "    #initialized as an ODE Function\n",
        "    #count the time\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "    #foorward pass \n",
        "    #input the ODE function and input data into the ODE Solver (adjoint method)\n",
        "    # to compute a forward pass\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=1e-7, atol=1e-9)\n",
        "        return out[1]#.double()\n",
        "\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value\n",
        "\n",
        "\n",
        "class SimpleODEfunc(nn.Module):\n",
        "  def __init__(self, ni):   #ni = size of input sample (number of variabels)\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(ni, 128)\n",
        "    self.bn1 = nn.BatchNorm1d(128)\n",
        "    self.linear2 = nn.Linear(128, 128)\n",
        "    self.bn2 = nn.BatchNorm1d(128)\n",
        "    self.linear3 = nn.Linear(128, ni)\n",
        "    self.bn3 = nn.BatchNorm1d(ni)\n",
        "    self.nfe = 0\n",
        "\n",
        "\n",
        "  def forward(self, t, x):\n",
        "    self.nfe += 1\n",
        "    x = F.tanh(self.bn1(self.linear1(x)))\n",
        "    x = F.tanh(self.bn2(self.linear2(x)))\n",
        "    x = F.tanh(self.bn3(self.linear3(x)))\n",
        "    return x\n",
        " \n",
        " \n",
        "def simpleODENetwork(n_fet):\n",
        "  # Initialize the network as 1 ODE Block\n",
        "  feature_layers = [ODEBlock(SimpleODEfunc(n_fet))]\n",
        "      \n",
        "  # Fully connected Layer at the end\n",
        "  fc_layers = [nn.Flatten(), nn.Linear(n_fet, 1)]\n",
        "      \n",
        "  #The Model consists of an ODE Block, and a fully connected block at the end\n",
        "  model = nn.Sequential(*feature_layers, *fc_layers).to(device)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHPe-7fJH5ni"
      },
      "source": [
        "**Check the ODEBlock works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzZxSwrrTWgV",
        "outputId": "e8cb222a-f489-4b2d-f6d7-eb36bed533a6"
      },
      "source": [
        "# Initialize the network as 1 ODE Block\n",
        "feature_layers = ODEBlock(SimpleODEfunc(10))\n",
        "\n",
        "# create data\n",
        "data = np.random.rand(4,10)\n",
        "data = torch.from_numpy(data)\n",
        "\n",
        "x = feature_layers(data.float())\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4041,  0.9776, -0.4789,  0.6597,  0.4691,  1.0897, -0.3951, -0.3704,\n",
            "          1.3748,  0.3938],\n",
            "        [ 0.3137,  0.5640,  0.9154,  0.8099,  0.8806,  0.2330,  0.9355,  0.6906,\n",
            "          0.7094, -0.1164],\n",
            "        [ 1.6727,  0.2565,  0.9801,  1.0312, -0.7975,  0.7739, -0.0282, -0.5505,\n",
            "         -0.6788,  1.2986],\n",
            "        [ 0.9558, -0.1448,  0.1940,  0.1581,  0.5787,  0.3549,  1.4007,  0.7104,\n",
            "          0.6486,  0.5312]], grad_fn=<SelectBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l81FHu6xH86j"
      },
      "source": [
        "**Define a function to generate data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaxoaXTRHV7j"
      },
      "source": [
        "def row_gen(rows):\n",
        "  # initialise lists of data ys=f(xs)\n",
        "  xs, ys = [],[]\n",
        " \n",
        "  for _ in range(rows):\n",
        "    x= [random.uniform(0,1) for i in range(9)]  # list of 9 random floats from (0,1)\n",
        "    y = ( (x[0]-x[1])**2 + (x[2] - x[3])**2 )**0.5 #only the 4 first xs are used to create y\n",
        "    xs.append(x) #adds 9 xs\n",
        "    ys.append(y) #adds 1 y\n",
        "  \n",
        "  xs = np.array(xs, dtype=\"Float64\")\n",
        "  ys = np.array(ys)\n",
        "\n",
        "  xs = torch.from_numpy(xs).to(device)\n",
        "  ys = torch.from_numpy(ys).to(device)\n",
        "\n",
        "  return xs, ys "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8J_cxsu4iDG"
      },
      "source": [
        "# create data\n",
        "xs, ys  = row_gen(10000)\n",
        "\n",
        "model = simpleODENetwork(10) #?? how do I initialize net?\n",
        "\n",
        "#Declare Gradient Descent Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "#Training Loop\n",
        "epochs = 20 \n",
        "batches_per_epoch = 250\n",
        "loss_dict = {}\n",
        "\n",
        "pbar = tq.tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    pbar.set_description(f'Calculating...epoch{epoch}/{epochs}')\n",
        "    temp_loss = 0\n",
        "    loss_dict[f'Epoch{epoch}'] = []\n",
        "\n",
        "    for batch_id in range(batches_per_epoch):\n",
        "        print(f'Epoch{epoch}: Batch {batch_id}/{batches_per_epoch}')\n",
        "        #init the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Input Training data to model, get Prediction\n",
        "        predictions = model(x.float())\n",
        "\n",
        "        #Compute Error using Prediction vs Actual Label\n",
        "        loss = rmse_loss(predictions, y)\n",
        "        temp_loss += loss.item()\n",
        "        loss_dict[f'Epoch{epoch}'].append(loss.item)\n",
        "\n",
        "        #Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print( f\"epoch num {epoch}, loss: {round(temp_loss/batches_per_epoch,2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3tk0O7o-NVd"
      },
      "source": [
        "## Re-writing code \n",
        "Saving to drive a new version of:\n",
        "*   S_NN_train.py\n",
        "*   S_NN_eval.py\n",
        "*   S_symmetry.py\n",
        "*   S_separability.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJC28l61W-u"
      },
      "source": [
        "### S_NN_train.py\n",
        "Has 3 parts: Imports, Auxilary functions, and NN_train function. I might have to add to imports, and definetly have to change NN_train to include the ODE classes instead of SimpleNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Z55FcGeh9X"
      },
      "source": [
        "####### imports ####################\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.utils.data as utils\n",
        "import time\n",
        "import os\n",
        "\n",
        "bs = 2048  #batch size\n",
        "wd = 1e-2  #seems uneeded\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "####### Auxilary functions #########\n",
        "class MultDataset(data.Dataset): #seems uneeded\n",
        "    def __init__(self, factors, product):\n",
        "        'Initialization'\n",
        "        self.factors = factors\n",
        "        self.product = product\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.product)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        x = self.factors[index]\n",
        "        y = self.product[index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "####### training function ############\n",
        "def NN_train(pathdir, filename, epochs=1000, lrs=1e-2, N_red_lr=4, pretrained_path=\"\"):\n",
        "    # figure out if there's a pretrained net\n",
        "    try:\n",
        "        os.mkdir(\"results/NN_trained_models/\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        os.mkdir(\"results/NN_trained_models/models/\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # initialize parameters to train\n",
        "    try:\n",
        "        n_variables = np.loadtxt(pathdir+\"%s\" %filename, dtype='str').shape[1]-1 #the number of columns in the txt minus 1 (y column)\n",
        "        variables = np.loadtxt(pathdir+\"%s\" %filename, usecols=(0,))  #load the the 1st column. this can be the only variable left\n",
        "\n",
        "        # set epochs accourding to learning rate\n",
        "        epochs = epochs//N_red_lr\n",
        "        epochs = int(epochs)\n",
        "\n",
        "        # check if there is only 1 variable in the data. If true than this is solved!\n",
        "        if n_variables==0 or n_variables==1:\n",
        "            print(\"Solved!\")#, variables[0])\n",
        "            return 0\n",
        "\n",
        "        # if not, continue to add variable columns to variables array\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+\"%s\" %filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v)) #add the rest of the variabeles columns\n",
        "\n",
        "        # create a vector with ?\n",
        "        f_dependent = np.loadtxt(pathdir+\"%s\" %filename, usecols=(n_variables,)) #load all the variabeles column\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1)) # shape it as a vector (n_rows,1)\n",
        "\n",
        "        # create a tensor in the shape (number of rows in data,n_variables)\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        # create a tensor of ?\n",
        "        product = torch.from_numpy(f_dependent) \n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "\n",
        "        # # create the NN\n",
        "        # class SimpleNet(nn.Module):\n",
        "        #     def __init__(self, ni):   #ni = size of input sample (number of variabels)\n",
        "        #         super().__init__()\n",
        "        #         self.linear1 = nn.Linear(ni, 128)\n",
        "        #         self.bn1 = nn.BatchNorm1d(128)\n",
        "        #         self.linear2 = nn.Linear(128, 128)\n",
        "        #         self.bn2 = nn.BatchNorm1d(128)\n",
        "        #         self.linear3 = nn.Linear(128, 64)\n",
        "        #         self.bn3 = nn.BatchNorm1d(64)\n",
        "        #         self.linear4 = nn.Linear(64,64)\n",
        "        #         self.bn4 = nn.BatchNorm1d(64)\n",
        "        #         self.linear5 = nn.Linear(64,1)\n",
        "            \n",
        "        #     def forward(self, x):\n",
        "        #         x = F.tanh(self.bn1(self.linear1(x)))\n",
        "        #         x = F.tanh(self.bn2(self.linear2(x)))\n",
        "        #         x = F.tanh(self.bn3(self.linear3(x)))\n",
        "        #         x = F.tanh(self.bn4(self.linear4(x)))\n",
        "        #         x = self.linear5(x)\n",
        "        #         return x\n",
        "\n",
        "        # create dataset and loader\n",
        "        my_dataset = utils.TensorDataset(factors,product) # create dataset\n",
        "        my_dataloader = utils.DataLoader(my_dataset, batch_size=bs, shuffle=True) # create your dataloader\n",
        "\n",
        "\n",
        "        # initialize model\n",
        "        if is_cuda:\n",
        "            # model_feynman = SimpleNet(n_variables).cuda()#old\n",
        "            model_feynman = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model_feynman = SimpleNet(n_variables)#old\n",
        "            model_feynman = simpleODENetwork(n_variables)\n",
        "\n",
        "        # if model was pretrained, then load the weights\n",
        "        if pretrained_path!=\"\":\n",
        "            model_feynman.load_state_dict(torch.load(pretrained_path))\n",
        "\n",
        "        # initialise loss to be the max number of rows in data\n",
        "        check_es_loss = 10000\n",
        "        \n",
        "        # start training loop\n",
        "        for i_i in range(N_red_lr):\n",
        "            #create Adam optimizer with current learning rate\n",
        "            optimizer_feynman = optim.Adam(model_feynman.parameters(), lr = lrs)\n",
        "\n",
        "            # for epoch in tot epochs defined in the train func / N_red_lr\n",
        "            for epoch in range(epochs):\n",
        "                model_feynman.train()\n",
        "\n",
        "                for i, data in enumerate(my_dataloader):\n",
        "                    optimizer_feynman.zero_grad()\n",
        "                \n",
        "                    if is_cuda:\n",
        "                        fct = data[0].float().cuda()  #number of variables?????????????\n",
        "                        prd = data[1].float().cuda()\n",
        "                    else:\n",
        "                        fct = data[0].float()\n",
        "                        prd = data[1].float()\n",
        "                    \n",
        "                    # calculate loos between what the model predicted and the actual variable value\n",
        "                    if i==2048: print(i)\n",
        "                    \n",
        "                    loss = rmse_loss(model_feynman(fct),prd) #this is define at the top\n",
        "                    loss.backward()\n",
        "                    optimizer_feynman.step()\n",
        "\n",
        "                # Early stopping\n",
        "                if epoch%20==0 and epoch>0:\n",
        "                    if check_es_loss < loss:\n",
        "                        break\n",
        "                    else:\n",
        "                        torch.save(model_feynman.state_dict(), \"results/NN_trained_models/models/\" + filename + \".h5\")\n",
        "                        check_es_loss = loss\n",
        "                if epoch==0:\n",
        "                    if check_es_loss < loss:\n",
        "                        torch.save(model_feynman.state_dict(), \"results/NN_trained_models/models/\" + filename + \".h5\")\n",
        "                        check_es_loss = loss\n",
        "        \n",
        "            print(loss)\n",
        "            lrs = lrs/10\n",
        "\n",
        "        return 1\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error in file: %s\" %filename)\n",
        "        raise\n",
        "#-------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVGH2DPnYMTn"
      },
      "source": [
        "# import os\n",
        "# os.mkdir(\"./results\")\n",
        "# os.mkdir(\"results/NN_trained_models\")\n",
        "# os.mkdir(\"results/NN_trained_models/models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xno7lPS5a6t2"
      },
      "source": [
        "aifeynman.get_demos()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDm4LTiirrNo"
      },
      "source": [
        "NN_train('example_data/', \"example2.txt\", epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQSWOmXvGYcf"
      },
      "source": [
        "# ####### imports ####################\n",
        "# from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "# from matplotlib import pyplot as plt\n",
        "import torch.utils.data as utils\n",
        "# import time\n",
        "import os\n",
        "\n",
        "bs = 2048  #batch size\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "####### Loss function #########\n",
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom\n",
        "#-------------------------------------\n",
        " \n",
        "\n",
        "####### training function ############\n",
        "def NN_train_abalation(pathdir = 'example_data/', filename=\"example2.txt\", epochs=1000, lrs=1e-2, N_red_lr=4, BN_bool=True, l2_bool=True, l4_bool=True, tanh_bool=True):\n",
        "        global H\n",
        "\n",
        "        # set epochs accourding to learning rate\n",
        "        epochs = int(epochs//N_red_lr)\n",
        "\n",
        "        n_variables = np.loadtxt(pathdir+\"%s\" %filename, dtype='str').shape[1]-1 #the number of columns in the txt minus 1 (y column)\n",
        "        variables = np.loadtxt(pathdir+\"%s\" %filename, usecols=(0,))  #load the the 1st column. this can be the only variable left\n",
        "        # continue to add variable columns to variables array\n",
        "        for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+\"%s\" %filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v)) #add the rest of the variabeles columns\n",
        "\n",
        "        # create a vector with the variables\n",
        "        f_dependent = np.loadtxt(pathdir+\"%s\" %filename, usecols=(n_variables,)) #load all the variabeles column\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1)) # shape it as a vector (n_rows,1)\n",
        "\n",
        "        # create a tensor in the shape (number of rows in data,n_variables)\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda: factors = factors.float().cuda()\n",
        "        else:       factors = factors.float()\n",
        "\n",
        "        # create a tensor of ?\n",
        "        product = torch.from_numpy(f_dependent) \n",
        "        if is_cuda: product = product.float().cuda()\n",
        "        else:       product = product.float()\n",
        "\n",
        "        # create the modular NN\n",
        "        class SimpleNet(nn.Module):\n",
        "            def __init__(self, ni, BN_bool, l2_bool, l4_bool, tanh_bool):   #ni = size of input sample (number of variabels)\n",
        "                super().__init__()\n",
        "                self.linear1 = nn.Linear(ni, 128)\n",
        "                self.bn1 = nn.BatchNorm1d(128)\n",
        "                self.linear2 = nn.Linear(128, 128)\n",
        "                self.bn2 = nn.BatchNorm1d(128)\n",
        "                self.linear3 = nn.Linear(128, 64)\n",
        "                self.bn3 = nn.BatchNorm1d(64)\n",
        "                self.linear4 = nn.Linear(64,64)\n",
        "                self.bn4 = nn.BatchNorm1d(64)\n",
        "                self.linear5 = nn.Linear(64,1)\n",
        "\n",
        "                if not BN_bool: self.name = 'Without BN'\n",
        "                if not l2_bool: self.name = 'Without layer2'\n",
        "                if not l4_bool: self.name = 'Without layer2'\n",
        "                if not tanh_bool: self.name = 'ReLu activation'\n",
        "\n",
        "            \n",
        "            def forward(self, x):\n",
        "                x = self.linear1(x)\n",
        "                if BN_bool:   x = self.bn1(x)\n",
        "                if tanh_bool: x = F.tanh(x)\n",
        "                else:         x = F.relu(x)\n",
        "\n",
        "                if l2_bool:   x = self.linear2(x)\n",
        "                if BN_bool:   x = self.bn2(x)\n",
        "                if tanh_bool: x = F.tanh(x)\n",
        "                else:         x = F.relu(x)\n",
        "\n",
        "                x = self.linear3(x)\n",
        "                if BN_bool:   x = self.bn3(x)\n",
        "                if tanh_bool: x = F.tanh(x)\n",
        "                else:         x = F.relu(x)\n",
        "\n",
        "                if l4_bool:   x = self.linear4(x)\n",
        "                if BN_bool:   x = self.bn4(x)\n",
        "                if tanh_bool: x = F.tanh(x)\n",
        "                else:         x = F.relu(x)\n",
        "   \n",
        "                x = self.linear5(x)\n",
        "                return x\n",
        "\n",
        "        # create dataset and loader\n",
        "        my_dataset = utils.TensorDataset(factors,product) # create dataset\n",
        "        my_dataloader = utils.DataLoader(my_dataset, batch_size=bs, shuffle=True) # create your dataloader\n",
        "\n",
        "        # initialize model\n",
        "        if is_cuda:  model_feynman = SimpleNet(n_variables, BN_bool, l2_bool, l4_bool, tanh_bool).cuda()\n",
        "        else:        model_feynman = SimpleNet(n_variables, BN_bool, l2_bool, l4_bool, tanh_bool)\n",
        "\n",
        "        # initialise loss to be the max number of rows in data\n",
        "        check_es_loss = 10000\n",
        "        \n",
        "        # start training loop\n",
        "        for i_i in range(N_red_lr):\n",
        "            #create Adam optimizer with current learning rate\n",
        "            optimizer_feynman = optim.Adam(model_feynman.parameters(), lr = lrs)\n",
        "\n",
        "            # for epoch in tot epochs defined in the train func / N_red_lr\n",
        "            for epoch in range(epochs):\n",
        "                model_feynman.train()\n",
        "\n",
        "                # initialize trackers\n",
        "                totalTrainLoss, totalValLoss, trainCorrect, valCorrect = 0,0,0,0\n",
        "\n",
        "                for i, data in enumerate(my_dataloader):\n",
        "                    #optimizer_feynman.zero_grad() ??????????????????????????????????????????????\n",
        "                 \n",
        "                    if is_cuda:\n",
        "                        fct = data[0].float().cuda() \n",
        "                        prd = data[1].float().cuda()\n",
        "                    else:\n",
        "                        fct = data[0].float()\n",
        "                        prd = data[1].float()\n",
        "                    \n",
        "                    # calculate loos between what the model predicted and the actual variable value                  \n",
        "                    loss = rmse_loss(model_feynman(fct),prd) #this is define at the top\n",
        "                    loss.backward()\n",
        "                    optimizer_feynman.step()\n",
        "\n",
        "                    # add the loss to the total training loss so far and\n",
        "\t\t                # calculate the number of correct predictions\n",
        "                    totalTrainLoss += loss\n",
        "                    trainCorrect += (prd.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "                # Early stopping\n",
        "                if epoch%20==0 and epoch>0:\n",
        "                    print('epoch',epoch)\n",
        "                    if check_es_loss < loss:\n",
        "                        break\n",
        "                    else:\n",
        "                        torch.save(model_feynman.state_dict(), f\"{filename}.h5\")\n",
        "                        check_es_loss = loss\n",
        "                if epoch==0:\n",
        "                    if check_es_loss < loss:\n",
        "                        torch.save(model_feynman.state_dict(), f\"{filename}.h5\")\n",
        "                        check_es_loss = loss\n",
        "        \n",
        "\n",
        "                \n",
        "                # Enter evaluation mode and return the loss between model predictions and actual y\n",
        "                factors_val = torch.from_numpy(variables[int(5*len(variables)/6):int(len(variables))])\n",
        "                if is_cuda: factors_val = factors_val.float().cuda()\n",
        "                else:       factors_val = factors_val.float()\n",
        "   \n",
        "                product_val = torch.from_numpy(f_dependent[int(5*len(variables)/6):int(len(variables))])      \n",
        "                if is_cuda: product_val = product_val.float().cuda()\n",
        "                else:       product_val = product_val.float()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    model_feynman.eval()\n",
        "                    Loss = rmse_loss(model_feynman(factors_val),product_val)\n",
        "\n",
        "                    totalValLoss += lossFn(pred, y)\n",
        "                    # calculate the number of correct predictions\n",
        "                    valCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            \n",
        "            \n",
        "            # calculate the average training and validation loss\n",
        "            avgTrainLoss = totalTrainLoss / trainSteps\n",
        "            avgValLoss = totalValLoss / valSteps\n",
        "\t          # calculate the training and validation accuracy\n",
        "            trainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
        "            valCorrect = valCorrect / len(valDataLoader.dataset)\n",
        "\t          # update our training history\n",
        "            H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "            H[\"train_acc\"].append(trainCorrect)\n",
        "            H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "            H[\"val_acc\"].append(valCorrect)\n",
        "            \n",
        "            \n",
        "            lrs = lrs/10\n",
        "        \n",
        "\n",
        "\n",
        "        return H\n",
        "#-------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "V23h4y0POdIF",
        "outputId": "2f92a751-8b1e-4b24-eee0-e45e058f4be1"
      },
      "source": [
        "# dict to store loss\n",
        "H = {\n",
        "\t\"train_loss\": [],\n",
        "\t\"train_acc\": [],\n",
        "\t\"val_loss\": [],\n",
        "\t\"val_acc\": []\n",
        "}\n",
        "\n",
        "for i in range(4):\n",
        "  if i==0:   H_withoutBN = NN_train_abalation(BN_bool=False)\n",
        "  elif i==1: H_withoutL2 = NN_train_abalation(l2_bool=False)\n",
        "  elif i==2: H_withoutL4 = NN_train_abalation(l4_bool=False)\n",
        "  else:      H_with_ReLu = NN_train_abalation(tanh_bool=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d07b864633df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mH_withoutBN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_train_abalation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBN_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mH_withoutL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_train_abalation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mH_withoutL4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_train_abalation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml4_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d22f54f3f565>\u001b[0m in \u001b[0;36mNN_train_abalation\u001b[0;34m(pathdir, filename, epochs, lrs, N_red_lr, BN_bool, l2_bool, l4_bool, tanh_bool)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mN_red_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mn_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#the number of columns in the txt minus 1 (y column)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#load the the 1st column. this can be the only variable left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: example_data/example2.txt not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yn_uDqsSKR9"
      },
      "source": [
        "for name in ['Without BN', 'Without layer2', 'Without layer4', 'ReLu activation']:\n",
        "  data = [name+' - train', name+' test']\n",
        "  title = 'The loss of SimpleNet used '+ name\n",
        "  plt_df[data].plot(xlabel=\"Epoch\", ylabel=\"loss\", grid=True, figsize=(10, 5), title= title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqbba-94xPjL"
      },
      "source": [
        "### **S_NN_eval.py**\n",
        "יש פה עוד עבודה"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIPrsRG6xkDe"
      },
      "source": [
        "######### Imports #####################\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "bs = 2048\n",
        "#--------------------------------------\n",
        "\n",
        "######### Defining same Auxilary functions as in training\n",
        "\n",
        "class MultDataset(data.Dataset):\n",
        "    def __init__(self, factors, product):\n",
        "        'Initialization'\n",
        "        self.factors = factors\n",
        "        self.product = product\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.product)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        x = self.factors[index]\n",
        "        y = self.product[index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom\n",
        "\n",
        "#---------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "######## Evaluation ############################\n",
        "def NN_eval(pathdir,filename):\n",
        "    # like before, load variables and see if it's solved (only 1)\n",
        "    try:\n",
        "        n_variables = np.loadtxt(pathdir+filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==0:\n",
        "            return 0\n",
        "        elif n_variables==1:\n",
        "            variables = np.reshape(variables,(len(variables),1))\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "\n",
        "        # like before\n",
        "        f_dependent = np.loadtxt(pathdir+filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables[0:int(5*len(variables)/6)])\n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent[0:int(5*len(f_dependent)/6)])\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        factors_val = torch.from_numpy(variables[int(5*len(variables)/6):int(len(variables))])\n",
        "        if is_cuda:\n",
        "            factors_val = factors_val.cuda()\n",
        "        else:\n",
        "            factors_val = factors_val\n",
        "        factors_val = factors_val.float()\n",
        "        product_val = torch.from_numpy(f_dependent[int(5*len(variables)/6):int(len(variables))])      \n",
        "        if is_cuda:\n",
        "            product_val = product_val.cuda()\n",
        "        else:\n",
        "            product_val = product_val\n",
        "        product_val = product_val.float()\n",
        "\n",
        "\n",
        "        # initialize net class. NEEDS TO BE REPLACED!\n",
        "        # class SimpleNet(nn.Module):\n",
        "        #     def __init__(self, ni):\n",
        "        #         super().__init__()\n",
        "        #         self.linear1 = nn.Linear(ni, 128)\n",
        "        #         self.bn1 = nn.BatchNorm1d(128)\n",
        "        #         self.linear2 = nn.Linear(128, 128)\n",
        "        #         self.bn2 = nn.BatchNorm1d(128)\n",
        "        #         self.linear3 = nn.Linear(128, 64)\n",
        "        #         self.bn3 = nn.BatchNorm1d(64)\n",
        "        #         self.linear4 = nn.Linear(64,64)\n",
        "        #         self.bn4 = nn.BatchNorm1d(64)\n",
        "        #         self.linear5 = nn.Linear(64,1)\n",
        "            \n",
        "        #     def forward(self, x):\n",
        "        #         x = F.tanh(self.bn1(self.linear1(x)))\n",
        "        #         x = F.tanh(self.bn2(self.linear2(x)))\n",
        "        #         x = F.tanh(self.bn3(self.linear3(x)))\n",
        "        #         x = F.tanh(self.bn4(self.linear4(x)))\n",
        "        #         x = self.linear5(x)\n",
        "        #         return x  \n",
        "\n",
        "        # Initialize model and load trained weights\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "                    \n",
        "        model.load_state_dict(torch.load(\"results/NN_trained_models/models/\"+filename+\".h5\"))\n",
        "\n",
        "        # Enter evaluation mode and return the loss between model predictions and actual y\n",
        "        model.eval()\n",
        "        return(rmse_loss(model(factors_val),product_val))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-6_Ed1gyQd3"
      },
      "source": [
        "### **S_symmetry.py**\n",
        "יש פה עוד עבודה"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCVYwhWT5eop"
      },
      "source": [
        "# checks for symmetries in the data\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from matplotlib import pyplot as plt\n",
        "from S_remove_input_neuron import remove_input_neuron\n",
        "import time\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# class SimpleNet(nn.Module):\n",
        "#     def __init__(self, ni):\n",
        "#         super().__init__()\n",
        "#         self.linear1 = nn.Linear(ni, 128)\n",
        "#         self.bn1 = nn.BatchNorm1d(128)\n",
        "#         self.linear2 = nn.Linear(128, 128)\n",
        "#         self.bn2 = nn.BatchNorm1d(128)\n",
        "#         self.linear3 = nn.Linear(128, 64)\n",
        "#         self.bn3 = nn.BatchNorm1d(64)\n",
        "#         self.linear4 = nn.Linear(64,64)\n",
        "#         self.bn4 = nn.BatchNorm1d(64)\n",
        "#         self.linear5 = nn.Linear(64,1)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = F.tanh(self.bn1(self.linear1(x)))\n",
        "#         x = F.tanh(self.bn2(self.linear2(x)))\n",
        "#         x = F.tanh(self.bn3(self.linear3(x)))\n",
        "#         x = F.tanh(self.bn4(self.linear4(x)))\n",
        "#         x = self.linear5(x)\n",
        "#         return x\n",
        "\n",
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom\n",
        "\n",
        "# checks if f(x,y)=f(x-y)\n",
        "def check_translational_symmetry_minus(pathdir, filename):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+\"/%s\" %filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD \\n\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "\n",
        "        with torch.no_grad():            \n",
        "            # make the shift x->x+a for 2 variables at a time (different variables)\n",
        "            min_error = 1000\n",
        "            best_i = -1\n",
        "            best_j = -1\n",
        "            for i in range(0,n_variables,1):\n",
        "                for j in range(0,n_variables,1):\n",
        "                    if i<j:\n",
        "                        fact_translate = factors.clone()\n",
        "                        a = 0.5*min(torch.std(fact_translate[:,i]),torch.std(fact_translate[:,j]))\n",
        "                        fact_translate[:,i] = fact_translate[:,i] + a\n",
        "                        fact_translate[:,j] = fact_translate[:,j] + a\n",
        "                        error = torch.median(abs(product-model(fact_translate)))\n",
        "                        if error<min_error:\n",
        "                            min_error = error\n",
        "                            best_i = i\n",
        "                            best_j = j\n",
        "        return min_error, best_i, best_j\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1,-1)\n",
        "    \n",
        "def do_translational_symmetry_minus(pathdir, filename, i,j):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+\"/%s\" %filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(0,))\n",
        "\n",
        "        for k in range(1,n_variables):\n",
        "            v = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(k,))\n",
        "            variables = np.column_stack((variables,v))\n",
        "        \n",
        "        f_dependent = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            #model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            #model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "\n",
        "        with torch.no_grad():   \n",
        "            file_name = filename + \"-translated_minus\"\n",
        "            ct_median = torch.median(torch.from_numpy(variables[:,j]))\n",
        "            data_translated = variables\n",
        "            data_translated[:,i] = variables[:,i]-variables[:,j]\n",
        "            data_translated =  np.delete(data_translated, j, axis=1)\n",
        "            data_translated = np.column_stack((data_translated,f_dependent))\n",
        "            try:\n",
        "                os.mkdir(\"results/translated_data_minus/\")\n",
        "            except:\n",
        "                pass\n",
        "            np.savetxt(\"results/translated_data_minus/\"+file_name , data_translated)\n",
        "            remove_input_neuron(model,n_variables,j,ct_median,\"results/NN_trained_models/models/\"+filename + \"-translated_minus_pretrained.h5\")\n",
        "            return (\"results/translated_data_minus/\",file_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1)\n",
        "        \n",
        "\n",
        "# checks if f(x,y)=f(x/y)\n",
        "def check_translational_symmetry_divide(pathdir, filename):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+\"/%s\" %filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD \\n\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+\"/%s\" %filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            a = 1.2\n",
        "            min_error = 1000\n",
        "            best_i = -1\n",
        "            best_j = -1\n",
        "            # make the shift x->x*a and y->y*a for 2 variables at a time (different variables)\n",
        "            for i in range(0,n_variables,1):\n",
        "                for j in range(0,n_variables,1):\n",
        "                    if i<j:\n",
        "                        fact_translate = factors.clone()\n",
        "                        fact_translate[:,i] = fact_translate[:,i]*a\n",
        "                        fact_translate[:,j] = fact_translate[:,j]*a\n",
        "                        error = torch.median(abs(product-model(fact_translate)))\n",
        "                        if error<min_error:\n",
        "                            min_error = error\n",
        "                            best_i = i\n",
        "                            best_j = j\n",
        "        return min_error, best_i, best_j                        \n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ILIRdMzN3E"
      },
      "source": [
        "### **S_separability.py**\n",
        "יש פה עוד עבודה"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_R3-foR6BQR"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import combinations\n",
        "import time\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# class SimpleNet(nn.Module):\n",
        "#     def __init__(self, ni):\n",
        "#         super().__init__()\n",
        "#         self.linear1 = nn.Linear(ni, 128)\n",
        "#         self.bn1 = nn.BatchNorm1d(128)\n",
        "#         self.linear2 = nn.Linear(128, 128)\n",
        "#         self.bn2 = nn.BatchNorm1d(128)\n",
        "#         self.linear3 = nn.Linear(128, 64)\n",
        "#         self.bn3 = nn.BatchNorm1d(64)\n",
        "#         self.linear4 = nn.Linear(64,64)\n",
        "#         self.bn4 = nn.BatchNorm1d(64)\n",
        "#         self.linear5 = nn.Linear(64,1)\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = F.tanh(self.bn1(self.linear1(x)))\n",
        "#         x = F.tanh(self.bn2(self.linear2(x)))\n",
        "#         x = F.tanh(self.bn3(self.linear3(x)))\n",
        "#         x = F.tanh(self.bn4(self.linear4(x)))\n",
        "#         x = self.linear5(x)\n",
        "#         return x  \n",
        "\n",
        "def rmse_loss(pred, targ):\n",
        "    denom = targ**2\n",
        "    denom = torch.sqrt(denom.sum()/len(denom))\n",
        "    return torch.sqrt(F.mse_loss(pred, targ))/denom\n",
        "\n",
        "def check_separability_plus(pathdir, filename):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        # make some variables at the time equal to the median of factors\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fact_vary = factors.clone()\n",
        "            for k in range(len(factors[0])):\n",
        "                fact_vary[:,k] = torch.full((len(factors),),torch.median(factors[:,k]))\n",
        "\n",
        "            # loop through all indices combinations\n",
        "            var_indices_list = np.arange(0,n_variables,1)\n",
        "            min_error = 1000\n",
        "            best_i = []\n",
        "            best_j = []\n",
        "            for i in range(1,n_variables):\n",
        "                c = combinations(var_indices_list, i)\n",
        "                for j in c:\n",
        "                    fact_vary_one = factors.clone()\n",
        "                    fact_vary_rest = factors.clone()\n",
        "                    rest_indx = list(filter(lambda x: x not in j, var_indices_list))\n",
        "                    for t1 in rest_indx:\n",
        "                        fact_vary_one[:,t1] = torch.full((len(factors),),torch.median(factors[:,t1]))\n",
        "                    for t2 in j:\n",
        "                        fact_vary_rest[:,t2] = torch.full((len(factors),),torch.median(factors[:,t2]))\n",
        "                    # check if the equation is separable\n",
        "                    sm = model(fact_vary_one)+model(fact_vary_rest)\n",
        "                    #error = torch.sqrt(torch.mean((product-sm+model(fact_vary))**2))/torch.sqrt(torch.mean(product**2))\n",
        "                    error = 2*torch.median(abs(product-sm+model(fact_vary)))\n",
        "                    if error<min_error:\n",
        "                        min_error = error\n",
        "                        best_i = j\n",
        "                        best_j = rest_indx\n",
        "                        \n",
        "        return min_error, best_i, best_j\n",
        "                    \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1,-1)                    \n",
        "                    \n",
        "                                           \n",
        "def do_separability_plus(pathdir, filename, list_i,list_j):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables) \n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        # make some variables at the time equal to the median of factors\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "        \n",
        "        fact_vary = factors.clone()\n",
        "        for k in range(len(factors[0])):\n",
        "            fact_vary[:,k] = torch.full((len(factors),),torch.median(factors[:,k]))\n",
        "        fact_vary_one = factors.clone()\n",
        "        fact_vary_rest = factors.clone()\n",
        "        for t1 in list_j:\n",
        "            fact_vary_one[:,t1] = torch.full((len(factors),),torch.median(factors[:,t1]))\n",
        "        for t2 in list_i:\n",
        "            fact_vary_rest[:,t2] = torch.full((len(factors),),torch.median(factors[:,t2]))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            str1 = filename+\"-add_a\"\n",
        "            str2 = filename+\"-add_b\"\n",
        "            # save the first half\n",
        "            data_sep_1 = variables\n",
        "            data_sep_1 = np.delete(data_sep_1,list_j,axis=1)\n",
        "            data_sep_1 = np.column_stack((data_sep_1,model(fact_vary_one).cpu()))\n",
        "            # save the second half  \n",
        "            data_sep_2 = variables\n",
        "            data_sep_2 = np.delete(data_sep_2,list_i,axis=1)\n",
        "            data_sep_2 = np.column_stack((data_sep_2,model(fact_vary_rest).cpu()-model(fact_vary).cpu()))\n",
        "            try:\n",
        "                os.mkdir(\"results/separable_add/\")\n",
        "            except:\n",
        "                pass\n",
        "            np.savetxt(\"results/separable_add/\"+str1,data_sep_1)\n",
        "            np.savetxt(\"results/separable_add/\"+str2,data_sep_2)\n",
        "            # if it is separable, return the 2 new files created and the index of the column with the separable variable\n",
        "            return (\"results/separable_add/\",str1,\"results/separable_add/\",str2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "def check_separability_multiply(pathdir, filename):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+filename, usecols=(n_variables,))\n",
        "\n",
        "        # Pick only data which is close enough to the maximum value (5 times less or higher)                                                                   \n",
        "        max_output = np.max(abs(f_dependent))\n",
        "        use_idx = np.where(abs(f_dependent)>=max_output/5)\n",
        "        f_dependent = f_dependent[use_idx]\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "        variables = variables[use_idx]\n",
        "\n",
        "        factors = torch.from_numpy(variables)\n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables)\n",
        "\n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        # make some variables at the time equal to the median of factors\n",
        "        models_one = []\n",
        "        models_rest = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fact_vary = factors.clone()\n",
        "            for k in range(len(factors[0])):\n",
        "                fact_vary[:,k] = torch.full((len(factors),),torch.median(factors[:,k]))\n",
        "\n",
        "            # loop through all indices combinations\n",
        "            var_indices_list = np.arange(0,n_variables,1)\n",
        "            min_error = 1000\n",
        "            best_i = []\n",
        "            best_j = []\n",
        "            for i in range(1,n_variables):\n",
        "                c = combinations(var_indices_list, i)\n",
        "                for j in c:\n",
        "                    fact_vary_one = factors.clone()\n",
        "                    fact_vary_rest = factors.clone()\n",
        "                    rest_indx = list(filter(lambda x: x not in j, var_indices_list))\n",
        "                    for t1 in rest_indx:\n",
        "                        fact_vary_one[:,t1] = torch.full((len(factors),),torch.median(factors[:,t1]))\n",
        "                    for t2 in j:\n",
        "                        fact_vary_rest[:,t2] = torch.full((len(factors),),torch.median(factors[:,t2]))\n",
        "                    # check if the equation is separable\n",
        "                    pd = model(fact_vary_one)*model(fact_vary_rest)\n",
        "                    #error = torch.sqrt(torch.mean((product-pd/model(fact_vary))**2))/torch.sqrt(torch.mean(product**2))\n",
        "                    error = 2*torch.median(abs(product-pd/model(fact_vary)))\n",
        "                    if error<min_error:\n",
        "                        min_error = error\n",
        "                        best_i = j\n",
        "                        best_j = rest_indx\n",
        "                        \n",
        "        return min_error, best_i, best_j\n",
        "                    \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1,-1)                         \n",
        "\n",
        "                    \n",
        "                    \n",
        "def do_separability_multiply(pathdir, filename, list_i,list_j):\n",
        "    try:\n",
        "        pathdir_weights = \"results/NN_trained_models/models/\"\n",
        "\n",
        "        # load the data\n",
        "        n_variables = np.loadtxt(pathdir+filename, dtype='str').shape[1]-1\n",
        "        variables = np.loadtxt(pathdir+filename, usecols=(0,))\n",
        "\n",
        "        if n_variables==1:\n",
        "            print(filename, \"just one variable for ADD\")\n",
        "            # if there is just one variable you have nothing to separate\n",
        "            return (-1,-1,-1)\n",
        "        else:\n",
        "            for j in range(1,n_variables):\n",
        "                v = np.loadtxt(pathdir+filename, usecols=(j,))\n",
        "                variables = np.column_stack((variables,v))\n",
        "        \n",
        "\n",
        "        f_dependent = np.loadtxt(pathdir+filename, usecols=(n_variables,))\n",
        "        f_dependent = np.reshape(f_dependent,(len(f_dependent),1))\n",
        "\n",
        "        factors = torch.from_numpy(variables)\n",
        "        if is_cuda:\n",
        "            factors = factors.cuda()\n",
        "        else:\n",
        "            factors = factors\n",
        "        factors = factors.float()\n",
        "\n",
        "        product = torch.from_numpy(f_dependent)\n",
        "        if is_cuda:\n",
        "            product = product.cuda()\n",
        "        else:\n",
        "            product = product\n",
        "        product = product.float()\n",
        "\n",
        "        # load the trained model and put it in evaluation mode\n",
        "        if is_cuda:\n",
        "            # model = SimpleNet(n_variables).cuda()\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "        else:\n",
        "            # model = SimpleNet(n_variables)\n",
        "            model = simpleODENetwork(n_variables).cuda()\n",
        "            \n",
        "        model.load_state_dict(torch.load(pathdir_weights+filename+\".h5\"))\n",
        "        model.eval()\n",
        "\n",
        "        # make some variables at the time equal to the median of factors\n",
        "        models_one = []\n",
        "        models_rest = []                    \n",
        "\n",
        "        fact_vary = factors.clone()\n",
        "        for k in range(len(factors[0])):\n",
        "            fact_vary[:,k] = torch.full((len(factors),),torch.median(factors[:,k]))\n",
        "        fact_vary_one = factors.clone()\n",
        "        fact_vary_rest = factors.clone()\n",
        "        for t1 in list_j:\n",
        "            fact_vary_one[:,t1] = torch.full((len(factors),),torch.median(factors[:,t1]))\n",
        "        for t2 in list_i:\n",
        "            fact_vary_rest[:,t2] = torch.full((len(factors),),torch.median(factors[:,t2]))        \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            str1 = filename+\"-mult_a\"\n",
        "            str2 = filename+\"-mult_b\"\n",
        "            # save the first half\n",
        "            data_sep_1 = variables\n",
        "            data_sep_1 = np.delete(data_sep_1,list_j,axis=1)\n",
        "            data_sep_1 = np.column_stack((data_sep_1,model(fact_vary_one).cpu()))\n",
        "            # save the second half  \n",
        "            data_sep_2 = variables\n",
        "            data_sep_2 = np.delete(data_sep_2,list_i,axis=1)\n",
        "            data_sep_2 = np.column_stack((data_sep_2,model(fact_vary_rest).cpu()/model(fact_vary).cpu()))\n",
        "            try:\n",
        "                os.mkdir(\"results/separable_mult/\")\n",
        "            except:\n",
        "                pass\n",
        "            np.savetxt(\"results/separable_mult/\"+str1,data_sep_1)\n",
        "            np.savetxt(\"results/separable_mult/\"+str2,data_sep_2)\n",
        "            # if it is separable, return the 2 new files created and the index of the column with the separable variable\n",
        "            return (\"results/separable_mult/\",str1,\"results/separable_mult/\",str2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return (-1,-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}